{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/coreymunk/Deep-Learning-CIFAR-100/blob/main/Simple_Neural_Net_CIFAR100.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UJa0dEM7Ng4"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from keras.datasets import cifar100\n",
        "\n",
        "# Load and preprocess CIFAR-100 data\n",
        "(x_train, y_train), (x_test, y_test) = cifar100.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2Bb-TPX4k9z",
        "outputId": "146fe77a-d48a-4f34-e75e-5e3f470f19b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration: 0, Accuracy: 0.0103\n",
            "Iteration: 100, Accuracy: 0.012\n",
            "Iteration: 200, Accuracy: 0.0122\n",
            "Iteration: 300, Accuracy: 0.0126\n"
          ]
        }
      ],
      "source": [
        "# Normalize the images\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Flatten the images\n",
        "x_train_flat = x_train.reshape(x_train.shape[0], -1)\n",
        "x_test_flat = x_test.reshape(x_test.shape[0], -1)\n",
        "\n",
        "# One-hot encoding of labels\n",
        "def one_hot_encode(y, classes=100): \n",
        "    return np.eye(classes)[y.reshape(-1)]\n",
        "\n",
        "y_train_one_hot = one_hot_encode(y_train)\n",
        "y_test_one_hot = one_hot_encode(y_test)\n",
        "\n",
        "# Initialize parameters\n",
        "def init_params():\n",
        "    number_of_input_features = 3072  # 32*32*3\n",
        "    y_classes = 100\n",
        "    num_hidden_neurons = 30  # This is adjustable\n",
        "\n",
        "    W1 = np.random.rand(num_hidden_neurons, number_of_input_features) - 0.5\n",
        "    b1 = np.random.rand(num_hidden_neurons, 1) - 0.5\n",
        "    W2 = np.random.rand(y_classes, num_hidden_neurons) - 0.5\n",
        "    b2 = np.random.rand(y_classes, 1) - 0.5\n",
        "\n",
        "    return W1, b1, W2, b2\n",
        "\n",
        "# Non-linearity functions\n",
        "def ReLU(Z):\n",
        "    return np.maximum(0, Z)\n",
        "\n",
        "def ReLU_deriv(Z):\n",
        "    return np.where(Z > 0, 1, 0)\n",
        "\n",
        "def softmax(Z):\n",
        "    A = np.exp(Z) / sum(np.exp(Z))\n",
        "    return A\n",
        "\n",
        "# Forward propagation\n",
        "def forward_prop(W1, b1, W2, b2, X):\n",
        "    Z1 = W1.dot(X) + b1\n",
        "    A1 = ReLU(Z1)\n",
        "    Z2 = W2.dot(A1) + b2\n",
        "    A2 = softmax(Z2)\n",
        "    return Z1, A1, Z2, A2\n",
        "\n",
        "# Backward propagation\n",
        "def backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y, m):\n",
        "    one_hot_Y = Y\n",
        "    dZ2 = A2 - one_hot_Y\n",
        "    dW2 = 1/m * dZ2.dot(A1.T)\n",
        "    db2 = 1/m * np.sum(dZ2, axis=1, keepdims=True)\n",
        "    dZ1 = W2.T.dot(dZ2) * ReLU_deriv(Z1)\n",
        "    dW1 = 1/m * dZ1.dot(X.T)\n",
        "    db1 = 1/m * np.sum(dZ1, axis=1, keepdims=True)\n",
        "    return dW1, db1, dW2, db2\n",
        "\n",
        "# Update parameters\n",
        "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n",
        "    W1 = W1 - alpha * dW1\n",
        "    b1 = b1 - alpha * db1\n",
        "    W2 = W2 - alpha * dW2\n",
        "    b2 = b2 - alpha * db2\n",
        "    return W1, b1, W2, b2\n",
        "\n",
        "# Helper functions for predictions and accuracy\n",
        "def get_predictions(A2):\n",
        "    return np.argmax(A2, 0)\n",
        "\n",
        "def get_accuracy(predictions, Y):\n",
        "    return np.sum(predictions == Y) / Y.size\n",
        "\n",
        "# Gradient descent function\n",
        "def gradient_descent(X, Y, alpha, iterations):\n",
        "    W1, b1, W2, b2 = init_params()\n",
        "    m = X.shape[1]  # Number of examples\n",
        "\n",
        "    for i in range(iterations):\n",
        "        Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X)\n",
        "        dW1, db1, dW2, db2 = backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y, m)\n",
        "        W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            predictions = get_predictions(A2)\n",
        "            accuracy = get_accuracy(predictions, Y.argmax(axis=0))\n",
        "            print(f\"Iteration: {i}, Accuracy: {accuracy}\")\n",
        "\n",
        "    return W1, b1, W2, b2\n",
        "\n",
        "# Training the model\n",
        "# Consider using a subset of the data for quick testing\n",
        "subset_size = 10000\n",
        "X_sub = x_train_flat[:subset_size].T  # Transpose for correct shape\n",
        "Y_sub = y_train_one_hot[:subset_size].T\n",
        "\n",
        "W1, b1, W2, b2 = gradient_descent(X_sub, Y_sub, alpha=0.25, iterations=400)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyM1h8LfgQ2Ze/R0InRLFB2t",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
